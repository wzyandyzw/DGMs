# 自回归模型（ARMs）知识点分类

## 知识点1：生成模型基础与概率分布

### 生成模型的基本概念
- 我们给定训练示例（如图像），希望学习概率分布 \( p(x) \) 使得：
  - 生成：从 \( p(x) \) 采样的新样本 \( x_{	ext{new}} \) 应与训练数据相似
  - 密度估计：\( p(x) \) 在训练数据相似的样本上值高，否则值低
  - 无监督表示学习：学习数据的共同特征

### 基本离散分布
- **伯努利分布**：用于二值随机变量，如（有偏）硬币翻转
  - 定义：\( X \sim 	ext{Ber}(p) \)，其中 \( \mathbb{P}(X = 	ext{Head}) = p \)，\( \mathbb{P}(X = 	ext{Tail}) = 1 - p \)
- **分类分布**：用于多类别随机变量，如（有偏）\( m \) 面骰子
  - 定义：\( X \sim 	ext{Cat}(p_1,...,p_m) \)，其中 \( \mathbb{P}(Y = i) = p_i \) 且 \( \sum_{i=1}^m p_i = 1 \)

### 联合分布的例子
- **单个像素颜色**：使用三个离散随机变量（红、绿、蓝通道）建模
- **二值图像**：使用 \( n \) 个伯努利变量建模，共有 \( 2^n \) 种可能状态

### 独立性与条件独立性
- **独立性**：若 \( X_1,...,X_n \) 独立，则 \( p(x_1,...,x_n) = p(x_1)...p(x_n) \)，但独立性假设通常太强
- **条件独立性**：若 \( X_{i+1} \perp X_1,...,X_{i-1}|X_i \)，则 \( p(x_1,...,x_n) = p(x_1)p(x_2|x_1)...p(x_n|x_{n-1}) \)

### 两个重要规则
- **链式法则**：\( p(S_1 \cap ... \cap S_n) = p(S_1)p(S_2|S_1)...p(S_n|S_1 \cap ... \cap S_{n-1}) \)
- **贝叶斯法则**：\( p(S_1|S_2) = \frac{p(S_1)p(S_2|S_1)}{p(S_2)} \)

## 知识点2：贝叶斯网络及其应用

### 贝叶斯网络的一般概念
- 使用条件参数化而非联合参数化
- 对每个随机变量 \( X_i \)，指定 \( p(x_i|x_{A_i}) \)，其中 \( \mathbf{X}_{A_i} \) 是一组随机变量
- 联合分布为 \( p(x_1,...,x_n) = \prod_i p(x_i|x_{A_i}) \)

### 贝叶斯网络的定义
- 由有向无环图（DAG）\( G = (V, E) \) 指定：
  - 每个节点 \( i \in V \) 对应一个随机变量 \( X_i \)
  - 每个节点有一个条件概率分布（CPD）\( p(x_i|\mathbf{x}_{	ext{Pa}(i)}) \)，指定变量在其父节点条件下的概率
- 联合分布定义为 \( p(x_1,...,x_n) = \prod_{i \in V} p(x_i|\mathbf{x}_{	ext{Pa}(i)}) \)

### 贝叶斯网络的例子
- **学生网络**：包含难度（Difficulty）、智力（Intelligence）、成绩（Grade）、SAT 分数和推荐信（Letter）等节点
- **朴素贝叶斯分类**：用于垃圾邮件分类，假设词在给定类别下条件独立
  - 模型：\( p(x_1,...,x_n,y) = p(y) \prod_{i=1}^n p(x_i|y) \)

### 逻辑回归与神经模型分类
- **逻辑回归**：使用参数化函数 \( p_\alpha(Y=1|\mathbf{x}) = \sigma(\alpha_0 + \sum_{i=1}^n \alpha_i x_i) \)，其中 \( \sigma(z) = \frac{1}{1+e^{-z}} \) 是 sigmoid 函数
- **神经模型**：使用非线性变换的输入特征 \( \mathbf{h}(\mathbf{x}; \mathbf{A}, \mathbf{b}) \)，然后应用 sigmoid 函数：\( p_{\alpha,\mathbf{A},\mathbf{b}}(Y=1|\mathbf{x}) = \sigma(\alpha_0 + \sum_j \alpha_j h_j) \)

## 知识点3：自回归模型基础

### 二值化MNIST的激励示例
- 给定二值化MNIST数据集（每个图像为 \( 28 	imes 28 = 784 \) 个二值像素）
- 目标：学习概率分布 \( p(\mathbf{x}) = p(x_1,...,x_{784}) \)，使得从 \( p(\mathbf{x}) \) 采样的样本看起来像手写数字
- 过程：
  1. 参数化模型族 \( \{p_	heta(\mathbf{x}) : 	heta \in \Theta\} \)
  2. 基于训练数据搜索模型参数 \( 	heta \)

### 自回归模型的定义
- 选择所有随机变量的顺序（如从左上角到右下角的光栅扫描顺序）
- 使用链式法则进行因式分解：\( p(x_1,...,x_n) = p(x_1)p(x_2|x_1)...p(x_n|x_1,...,x_{n-1}) \)
- 基于逻辑回归，假设：\( p(x_1,...,x_n) = p_{\alpha_1}(x_1)p_{\alpha_2}(x_2|x_1)...p_{\alpha_n}(x_n|x_1,...,x_{n-1}) \)
- 具体地：
  - \( p_{\alpha_1}(X_1 = 1) = \alpha_1 \)
  - \( p_{\alpha_2}(X_2 = 1|x_1) = \sigma(\alpha_{2,0} + \alpha_{2,1}x_1) \)
  - \( p_{\alpha_n}(X_n = 1|x_1,...,x_{n-1}) = \sigma(\alpha_{n,0} + \sum_{j=1}^{n-1}\alpha_{n,j}x_j) \)

## 知识点4：自回归模型的具体实现

### 完全可见的 sigmoid 信念网络（FVSBNs）
- 条件变量 \( X_i|X_1,...,X_{i-1} \) 是伯努利分布，其中：
  \( \hat{x}_i := p_{\alpha_i}(X_i = 1|x_1,...,x_{i-1}) = \sigma(\alpha_{i,0} + \sum_{j=1}^{i-1}\alpha_{i,j}x_j) \)
- 评估概率：如 \( p(X_1=0,X_2=1,X_3=1,X_4=0) = (1-\hat{x}_1)\hat{x}_2\hat{x}_3(1-\hat{x}_4) \)
- 采样方法：按顺序依次采样每个变量

### 神经自回归分布估计（NADE）
- 使用单层神经网络代替逻辑回归：
  \( \mathbf{h}_i = \sigma(\mathbf{A}_i\mathbf{x}_{<<i} + \mathbf{c}_i) \)，\( \hat{x}_i = \sigma(\alpha_i^	op \mathbf{h}_i + b_i) \)
- 通过共享权重减少参数数量并加速计算：
  \( \mathbf{h}_i = \sigma(\mathbf{W}_{<<i}\mathbf{x}_{<<i} + \mathbf{c}) \)，\( \hat{x}_i = \sigma(\alpha_i^	op \mathbf{h}_i + b_i) \)

### 一般离散分布与实值 NADE
- **一般离散分布**：对于非二值离散随机变量 \( X_i \in \{1,...,K\} \)，使用 softmax 函数参数化分类分布：
  \( \hat{\mathbf{x}}_i := [p_{i,1},...,p_{i,K}]^	op = 	ext{softmax}(\mathbf{D}_i\mathbf{h}_i + \mathbf{b}_i) \)
- **实值 NADE（RNADE）**：对于连续随机变量，使用高斯混合模型：
  \( p(x_i|x_1,...,x_{i-1}) = \sum_{j=1}^K \frac{1}{K} \mathcal{N}(x_i; \mu_{i,j}, \sigma^2_{i,j}) \)

## 知识点5：自回归模型与其他模型的比较

### ARMs 与自动编码器的比较
- **自动编码器**：包含编码器 \( E(\cdot) \) 和解码器 \( D(\cdot) \)，目标是 \( D(E(\mathbf{x})) \approx \mathbf{x} \)
- **ARMs**：定义了可采样的概率分布，而普通自动编码器不是生成模型
- 损失函数相似：对于二值随机变量，使用交叉熵损失

### MADE：用于分布估计的掩码自动编码器
- **挑战**：创建具有自回归（DAG）结构的自动编码器
- **解决方案**：使用掩码来禁止某些路径
- **实现**：
  1. 为每个隐藏层单元分配一个随机整数 \( i \in [1, n-1] \)，该单元仅允许依赖前 \( i \) 个输入
  2. 添加掩码以保持不变性：仅连接到前一层中分配的数字较小或相等的单元（最后一层中严格小于）

## 知识点6：基于序列的自回归模型

### 循环神经网络（RNNs）
- **挑战**：建模 \( p(x_t|x_{1:t-1}; \alpha_t) \)，历史长度不断增加
- **解决方案**：使用隐藏层 \( h_t \) 总结截至时间 \( t \) 为止的输入
- **更新规则**：
  - 摘要更新：\( h_{t+1} = 	anh(W_{hh}h_t + W_{xh}x_{t+1}) \)
  - 预测：\( o_{t+1} = W_{hy}h_{t+1} \)
  - 摘要初始化：\( h_0 = b_0 \)

### 字符 RNN 示例
- **应用**：生成文本（如莎士比亚作品、维基百科文章、婴儿名字）
- **过程**：字符级建模，一次生成一个字符
- **示例**：训练 3 层 RNN 生成类似莎士比亚风格的文本

### RNN 的问题
- 单个隐藏向量需要总结不断增长的历史
- 顺序评估，无法并行化
- 访问多步之前的信息时存在梯度爆炸/消失问题

## 知识点7：现代自回归模型

### 基于注意力的模型
- 使用注意力机制比较查询向量与一组键向量：
  1. 比较当前隐藏状态（查询）与所有过去的隐藏状态（键）
  2. 构建注意力分布，确定历史的哪些部分相关
  3. 构建历史摘要
  4. 使用摘要和当前隐藏状态预测下一个标记/单词

### 生成式 Transformer
- **当前最先进技术**：使用 Transformer 替代 RNN
- **特点**：
  - 注意力机制自适应地仅关注相关上下文
  - 避免递归计算，仅使用自注意力以实现并行化
  - 需要掩码自注意力以保持自回归结构
- **应用**：GPT 模型等大型语言模型